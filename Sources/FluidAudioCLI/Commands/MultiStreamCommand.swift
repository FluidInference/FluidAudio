#if os(macOS)
import AVFoundation
import FluidAudio
import Foundation


/// Command to demonstrate multi-stream ASR with shared model loading
@available(macOS 13.0, *)
enum MultiStreamCommand {
    
    static func run(arguments: [String]) async {
        // Parse arguments
        guard !arguments.isEmpty else {
            print("‚ùå No audio files specified")
            printUsage()
            exit(1)
        }
        
        let audioFile1 = arguments[0]
        var audioFile2: String? = nil
        var showDebug = false
        
        // Parse options
        var i = 1
        while i < arguments.count {
            switch arguments[i] {
            case "--debug":
                showDebug = true
            case "--help", "-h":
                printUsage()
                exit(0)
            default:
                // Check if it's a second audio file
                if audioFile2 == nil && arguments[i].hasSuffix(".wav") {
                    audioFile2 = arguments[i]
                } else {
                    print("‚ö†Ô∏è  Unknown option: \(arguments[i])")
                }
            }
            i += 1
        }
        
        // Use same file for both streams if only one provided
        let micAudioFile = audioFile1
        let systemAudioFile = audioFile2 ?? audioFile1
        
        print("üé§ Multi-Stream ASR Test")
        print("========================\n")
        
        if audioFile2 != nil {
            print("üìÅ Processing two different files:")
            print("  Microphone: \(micAudioFile)")
            print("  System: \(systemAudioFile)\n")
        } else {
            print("üìÅ Processing single file on both streams: \(audioFile1)\n")
        }
        
        do {
            // Load first audio file (microphone)
            let micFileURL = URL(fileURLWithPath: micAudioFile)
            let micFileHandle = try AVAudioFile(forReading: micFileURL)
            let micFormat = micFileHandle.processingFormat
            let micFrameCount = AVAudioFrameCount(micFileHandle.length)
            
            guard let micBuffer = AVAudioPCMBuffer(pcmFormat: micFormat, frameCapacity: micFrameCount) else {
                print("‚ùå Failed to create microphone audio buffer")
                return
            }
            try micFileHandle.read(into: micBuffer)
            
            // Load second audio file (system)
            let systemFileURL = URL(fileURLWithPath: systemAudioFile)
            let systemFileHandle = try AVAudioFile(forReading: systemFileURL)
            let systemFormat = systemFileHandle.processingFormat
            let systemFrameCount = AVAudioFrameCount(systemFileHandle.length)
            
            guard let systemBuffer = AVAudioPCMBuffer(pcmFormat: systemFormat, frameCapacity: systemFrameCount) else {
                print("‚ùå Failed to create system audio buffer")
                return
            }
            try systemFileHandle.read(into: systemBuffer)
            
            print("üìä Audio file info:")
            print("üéôÔ∏è Microphone file:")
            print("  Sample rate: \(micFormat.sampleRate) Hz")
            print("  Channels: \(micFormat.channelCount)")
            print("  Duration: \(String(format: "%.2f", Double(micFileHandle.length) / micFormat.sampleRate)) seconds")
            
            print("\nüíª System audio file:")
            print("  Sample rate: \(systemFormat.sampleRate) Hz")
            print("  Channels: \(systemFormat.channelCount)")
            print("  Duration: \(String(format: "%.2f", Double(systemFileHandle.length) / systemFormat.sampleRate)) seconds\n")
            
            // Create a streaming session
            print("üîÑ Creating streaming session...")
            let session = StreamingAsrSession()
            
            // Initialize models once
            print("üì¶ Loading ASR models (shared across streams)...")
            let startTime = Date()
            try await session.initialize()
            let loadTime = Date().timeIntervalSince(startTime)
            print("‚úÖ Models loaded in \(String(format: "%.2f", loadTime))s\n")
            
            // Create streams for different sources
            print("üéôÔ∏è Creating streams for different audio sources...")
            let micStream = try await session.createStream(
                source: .microphone,
                config: .default
            )
            print("‚úÖ Created microphone stream")
            
            let systemStream = try await session.createStream(
                source: .system,
                config: .default
            )
            print("‚úÖ Created system audio stream\n")
            
            // Listen for updates from both streams (only if debug enabled)
            let micTask = Task {
                if showDebug {
                    for await update in await micStream.transcriptionUpdates {
                        print("[MIC] \(update.isConfirmed ? "‚úì" : "~") \(update.text)")
                    }
                }
            }
            
            let systemTask = Task {
                if showDebug {
                    for await update in await systemStream.transcriptionUpdates {
                        print("[SYS] \(update.isConfirmed ? "‚úì" : "~") \(update.text)")
                    }
                }
            }
            
            print("üéµ Streaming audio files in parallel...")
            print("  Both streams using default config (10.0s chunks)\n")
            
            // Process both files in parallel
            let micProcessingTask = Task {
                await streamAudioFile(
                    buffer: micBuffer,
                    format: micFormat,
                    to: micStream,
                    label: "MIC",
                    showDebug: showDebug
                )
            }
            
            let systemProcessingTask = Task {
                await streamAudioFile(
                    buffer: systemBuffer,
                    format: systemFormat,
                    to: systemStream,
                    label: "SYS",
                    showDebug: showDebug
                )
            }
            
            // Wait for both to complete
            await micProcessingTask.value
            await systemProcessingTask.value
            
            print("‚è≥ Finalizing transcriptions...")
            
            // Get final results
            let micFinal = try await micStream.finish()
            let systemFinal = try await systemStream.finish()
            
            // Cancel update tasks
            micTask.cancel()
            systemTask.cancel()
            
            // Print results
            print("\n" + String(repeating: "=", count: 60))
            print("üìù TRANSCRIPTION RESULTS")
            print(String(repeating: "=", count: 60))
            
            print("\nüéôÔ∏è MICROPHONE STREAM:")
            print("\(micFinal)")
            
            print("\nüíª SYSTEM AUDIO STREAM:")
            print("\(systemFinal)")
            
            // Show active streams
            print("\nüîç Session info:")
            let activeStreams = await session.activeStreams
            print("Active streams: \(activeStreams.count)")
            for (source, stream) in activeStreams {
                print("  - \(source): \(await stream.source)")
            }
            
            // Cleanup
            await session.cleanup()
            print("\n‚úÖ Session cleaned up")
            
        } catch {
            print("‚ùå Error: \(error)")
        }
    }
    
    /// Helper function to stream an audio file to a stream
    private static func streamAudioFile(
        buffer: AVAudioPCMBuffer,
        format: AVAudioFormat,
        to stream: StreamingAsrManager,
        label: String,
        showDebug: Bool
    ) async {
        let chunkDuration = 0.5 // 500ms chunks
        let samplesPerChunk = Int(chunkDuration * format.sampleRate)
        var position = 0
        
        while position < Int(buffer.frameLength) {
            let remainingSamples = Int(buffer.frameLength) - position
            let chunkSize = min(samplesPerChunk, remainingSamples)
            
            // Create chunk buffer
            guard let chunkBuffer = AVAudioPCMBuffer(
                pcmFormat: format,
                frameCapacity: AVAudioFrameCount(chunkSize)
            ) else {
                break
            }
            
            // Copy samples
            for channel in 0..<Int(format.channelCount) {
                if let sourceData = buffer.floatChannelData?[channel],
                   let destData = chunkBuffer.floatChannelData?[channel] {
                    for i in 0..<chunkSize {
                        destData[i] = sourceData[position + i]
                    }
                }
            }
            chunkBuffer.frameLength = AVAudioFrameCount(chunkSize)
            
            // Stream the chunk
            await stream.streamAudio(chunkBuffer)
            
            if showDebug {
                let progress = Float(position) / Float(buffer.frameLength) * 100
                print("[\(label)] Progress: \(String(format: "%.1f", progress))%")
            }
            
            // Process as fast as possible
            
            position += chunkSize
        }
        
        if showDebug {
            print("[\(label)] Streaming complete")
        }
    }
    
    private static func printUsage() {
        print(
            """
            
            Multi-Stream Command Usage:
                fluidaudio multi-stream <audio_file1> [audio_file2] [options]
            
            Options:
                --debug            Show debug information
                --help, -h         Show this help message
            
            Examples:
                # Process same file on both streams
                fluidaudio multi-stream audio.wav
                
                # Process two different files in parallel
                fluidaudio multi-stream mic_audio.wav system_audio.wav
                
                # With debug output
                fluidaudio multi-stream audio1.wav audio2.wav --debug
                
            
            This command demonstrates:
            - Loading ASR models once and sharing across streams
            - Creating separate streams for microphone and system audio
            - Parallel transcription with shared resources
            """
        )
    }
}
#endif