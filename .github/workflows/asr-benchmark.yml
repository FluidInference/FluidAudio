name: ASR Benchmark

on:
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      max_files:
        description: 'Number of files to test per dataset'
        required: false
        default: '100'

jobs:
  asr-benchmark:
    name: ASR Benchmark
    runs-on: macos-14  # M1 Mac for Neural Engine support
    permissions:
      contents: read
      pull-requests: write
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Setup Swift
      uses: swift-actions/setup-swift@v2
      with:
        swift-version: "6.0"
    
    # Cache Swift Package Manager
    - name: Cache SPM
      uses: actions/cache@v4
      with:
        path: .build
        key: ${{ runner.os }}-spm-${{ hashFiles('Package.resolved') }}
        restore-keys: |
          ${{ runner.os }}-spm-
    
    # Cache Parakeet Models (CoreML packages)
    - name: Cache Parakeet Models
      uses: actions/cache@v4
      with:
        path: ~/Library/Application Support/FluidAudio/Models/Parakeet
        key: ${{ runner.os }}-parakeet-coreml-models-v4
        restore-keys: |
          ${{ runner.os }}-parakeet-coreml-models-
          ${{ runner.os }}-parakeet-models-
    
    # Cache LibriSpeech Dataset
    - name: Cache LibriSpeech Dataset
      uses: actions/cache@v4
      with:
        path: ~/Documents/Datasets/librispeech
        key: ${{ runner.os }}-librispeech-test-clean-v1
    
    # Build in release mode
    - name: Build
      run: swift build -c release
    
    # Download Parakeet models if not cached
    - name: Download Parakeet Models
      run: |
        echo "📥 === PARAKEET MODEL DOWNLOAD STEP ==="
        echo "📥 Ensuring Parakeet models are available..."
        MODELS_DIR="$HOME/Library/Application Support/FluidAudio/Models/Parakeet"
        mkdir -p "$MODELS_DIR"
        
        echo "📍 Models directory: $MODELS_DIR"
        
        # Check if all required model files exist
        if [ -d "$MODELS_DIR/Melspectogram.mlpackage" ] && \
           [ -d "$MODELS_DIR/ParakeetEncoder.mlpackage" ] && \
           [ -d "$MODELS_DIR/ParakeetDecoder.mlpackage" ] && \
           [ -d "$MODELS_DIR/RNNTJoint.mlpackage" ]; then
          echo "✅ All Parakeet models already cached"
          ls -la "$MODELS_DIR"
        else
          echo "📥 Downloading Parakeet models from Hugging Face..."
          
          # Download models from FluidInference/parakeet-tdt-0.6b-v2-coreml
          # Use git to clone the model repository
          echo "📥 Cloning model repository..."
          
          # Create a temporary directory for cloning
          TEMP_DIR=$(mktemp -d)
          cd "$TEMP_DIR"
          
          # Clone the repository with depth 1 to save time
          git clone --depth 1 https://huggingface.co/FluidInference/parakeet-tdt-0.6b-v2-coreml.git
          
          # Copy the mlpackage directories to the models directory
          cd parakeet-tdt-0.6b-v2-coreml
          for model in "Melspectogram" "ParakeetEncoder" "ParakeetDecoder" "RNNTJoint"; do
            if [ -d "${model}.mlpackage" ]; then
              echo "📦 Installing ${model}.mlpackage..."
              cp -r "${model}.mlpackage" "$MODELS_DIR/"
              echo "✅ Installed ${model}.mlpackage"
            else
              echo "❌ ${model}.mlpackage not found in repository"
            fi
          done
          
          # Clean up
          cd /
          rm -rf "$TEMP_DIR"
          
          # Verify all models are present
          if [ -d "$MODELS_DIR/Melspectogram.mlpackage" ] && \
             [ -d "$MODELS_DIR/ParakeetEncoder.mlpackage" ] && \
             [ -d "$MODELS_DIR/ParakeetDecoder.mlpackage" ] && \
             [ -d "$MODELS_DIR/RNNTJoint.mlpackage" ]; then
            echo "✅ All Parakeet models downloaded successfully"
            ls -la "$MODELS_DIR"
          else
            echo "❌ Failed to download all required models"
            echo "SKIP_BENCHMARK=true" >> $GITHUB_ENV
          fi
        fi
    
    # Determine test size based on changes
    - name: Determine Test Size
      id: test_size
      run: |
        # Default to 100 files for both datasets
        MAX_FILES="${{ github.event.inputs.max_files || '100' }}"
        
        # If only minor changes, do quick test
        if [[ "${{ github.event_name }}" == "pull_request" ]]; then
          # Fetch the base branch
          git fetch origin ${{ github.base_ref }}:refs/remotes/origin/${{ github.base_ref }} || true
          
          # Check what files changed
          CHANGED_FILES=$(git diff --name-only origin/${{ github.base_ref }}...HEAD 2>/dev/null || echo "")
          
          # If only docs/configs changed, do minimal test
          if echo "$CHANGED_FILES" | grep -qE "^(README|\.github/|docs/|.*\.md$)" && \
             ! echo "$CHANGED_FILES" | grep -qE "^Sources/"; then
            MAX_FILES=10
            echo "📝 Only documentation changed, running minimal test"
          # If ASR-related files changed, do full test
          elif echo "$CHANGED_FILES" | grep -qE "(ASR|Asr|asr|Parakeet|Transcript)"; then
            MAX_FILES=100
            echo "🔍 ASR-related changes detected, running comprehensive test"
          else
            MAX_FILES=50
            echo "📊 Standard test size for general changes"
          fi
        fi
        
        echo "MAX_FILES=$MAX_FILES" >> $GITHUB_OUTPUT
        echo "📊 Will test with $MAX_FILES files per dataset"
    
    # Run ASR benchmark on test-clean
    - name: Run ASR Benchmark - test-clean
      id: benchmark_clean
      if: env.SKIP_BENCHMARK != 'true'
      run: |
        MAX_FILES="${{ steps.test_size.outputs.MAX_FILES }}"
        echo "Running ASR benchmark on test-clean with $MAX_FILES files"
        
        swift run -c release fluidaudio asr-benchmark \
          --subset test-clean \
          --max-files "$MAX_FILES" \
          --output asr_results_clean.json 2>&1 | tee benchmark_clean.log
        
        # Extract WER metrics only
        # Note: RTFx metrics are not reliable in virtualized CI environments (GitHub Actions)
        # due to restricted Neural Engine access. See ASRManager.swift for details.
        WER_AVG=$(grep "Average WER:" benchmark_clean.log | grep -o '[0-9.]*' | head -1 || echo "0")
        WER_MED=$(grep "Median WER:" benchmark_clean.log | grep -o '[0-9.]*' | head -1 || echo "0")
        
        echo "CLEAN_WER_AVG=$WER_AVG" >> $GITHUB_OUTPUT
        echo "CLEAN_WER_MED=$WER_MED" >> $GITHUB_OUTPUT
    
    # Run ASR benchmark on test-other
    - name: Run ASR Benchmark - test-other
      id: benchmark_other
      if: env.SKIP_BENCHMARK != 'true'
      run: |
        MAX_FILES="${{ steps.test_size.outputs.MAX_FILES }}"
        echo "Running ASR benchmark on test-other with $MAX_FILES files"
        
        swift run -c release fluidaudio asr-benchmark \
          --subset test-other \
          --max-files "$MAX_FILES" \
          --output asr_results_other.json 2>&1 | tee benchmark_other.log
        
        # Extract WER metrics only
        # Note: RTFx metrics are not reliable in virtualized CI environments (GitHub Actions)
        # due to restricted Neural Engine access. See ASRManager.swift for details.
        WER_AVG=$(grep "Average WER:" benchmark_other.log | grep -o '[0-9.]*' | head -1 || echo "0")
        WER_MED=$(grep "Median WER:" benchmark_other.log | grep -o '[0-9.]*' | head -1 || echo "0")
        
        echo "OTHER_WER_AVG=$WER_AVG" >> $GITHUB_OUTPUT
        echo "OTHER_WER_MED=$WER_MED" >> $GITHUB_OUTPUT
    
    # Calculate combined metrics
    - name: Calculate Combined Metrics
      id: combined
      if: env.SKIP_BENCHMARK != 'true'
      run: |
        # Count files
        TOTAL_FILES=$(( ${{ steps.test_size.outputs.MAX_FILES }} * 2 ))
        
        echo "TOTAL_FILES=$TOTAL_FILES" >> $GITHUB_OUTPUT
    
    # Post comment with combined results
    - name: Comment PR with ASR Results
      if: github.event_name == 'pull_request' && always()
      continue-on-error: true
      uses: actions/github-script@v7
      with:
        script: |
          // Extract all metrics
          const cleanWerAvg = '${{ steps.benchmark_clean.outputs.CLEAN_WER_AVG }}' || 'N/A';
          const cleanWerMed = '${{ steps.benchmark_clean.outputs.CLEAN_WER_MED }}' || 'N/A';
          
          const otherWerAvg = '${{ steps.benchmark_other.outputs.OTHER_WER_AVG }}' || 'N/A';
          const otherWerMed = '${{ steps.benchmark_other.outputs.OTHER_WER_MED }}' || 'N/A';
          
          const totalFiles = '${{ steps.combined.outputs.TOTAL_FILES }}' || 'N/A';
          const filesPerDataset = '${{ steps.test_size.outputs.MAX_FILES }}';
          
          let body = `## 🔊 ASR Benchmark Results
          
          **Last Updated:** ${new Date().toISOString()}
          **Configuration:** test-clean & test-other | ${filesPerDataset} files per dataset | Total: ${totalFiles} files
          
          ### 📈 Performance Metrics
          
          | Metric | test-clean | test-other | Delta | Status |
          |--------|------------|------------|-------|--------|
          | **WER Average** | ${cleanWerAvg}% | ${otherWerAvg}% | +${(parseFloat(otherWerAvg) - parseFloat(cleanWerAvg)).toFixed(1)}% | ${parseFloat(cleanWerAvg) < 10 && parseFloat(otherWerAvg) < 20 ? '✅' : '⚠️'} |
          | **WER Median** | ${cleanWerMed}% | ${otherWerMed}% | +${(parseFloat(otherWerMed) - parseFloat(cleanWerMed)).toFixed(1)}% | ${parseFloat(cleanWerMed) < 5 && parseFloat(otherWerMed) < 10 ? '✅' : '⚠️'} |
          
          ### 🎯 Performance Targets
          
          | Target | Status | Result |
          |--------|--------|--------|
          | test-clean WER < 10% | ${parseFloat(cleanWerAvg) < 10 ? '✅ PASS' : '❌ FAIL'} | ${cleanWerAvg}% |
          | test-other WER < 20% | ${parseFloat(otherWerAvg) < 20 ? '✅ PASS' : '❌ FAIL'} | ${otherWerAvg}% |
          
          ### 📊 Analysis
          
          ${parseFloat(cleanWerAvg) < 6 ? 
            '🎉 **Excellent**: Research-grade accuracy on clean audio.' :
            parseFloat(cleanWerAvg) < 10 ? 
            '✅ **Good**: Meets industry standards for clean audio.' :
            '⚠️ **Needs Improvement**: Consider model tuning or preprocessing improvements.'
          }
          
          ${(parseFloat(otherWerAvg) - parseFloat(cleanWerAvg)) > 15 ? 
            '⚠️ **Noise Sensitivity**: Large gap between clean/noisy performance. Consider noise-robust training.' :
            (parseFloat(otherWerAvg) - parseFloat(cleanWerAvg)) < 10 ?
            '✅ **Excellent Robustness**: Minimal degradation on noisy audio.' :
            '📊 **Good Robustness**: Acceptable performance across conditions.'
          }
          
          ### ⚡ Performance Note
          
          **RTFx (Real-Time Factor) benchmarking is not available in CI environments.**
          
          GitHub Actions runners use virtualized M1 Macs where Neural Engine access is severely restricted, resulting in ~7x slower performance compared to bare metal. Local benchmarks on physical Apple Silicon show:
          - **M1/M2 Mac**: ~21x real-time (21x faster than real-time)
          - **GitHub Actions**: ~3x real-time (virtualization overhead)
          
          For accurate performance benchmarking, please run locally on Apple Silicon hardware.
          
          ---
          <sub>🤖 Automated ASR benchmark by FluidAudio CI</sub>
          
          <!-- fluidaudio-benchmark-asr -->`;
          
          // Find and update existing comment
          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
          });
          
          console.log(`Found ${comments.length} comments on PR`);
          
          const botComment = comments.find(comment => {
            // Check if it's a bot comment (GitHub Actions bot)
            const isBot = comment.user.type === 'Bot' || 
                         comment.user.login === 'github-actions[bot]' ||
                         comment.user.login.includes('[bot]');
            
            // Check if it contains our identifier
            const hasIdentifier = comment.body.includes('<!-- fluidaudio-benchmark-asr -->');
            
            // Also check for the header as a fallback (with emoji)
            const hasHeader = comment.body.includes('## 🔊 ASR Benchmark Results');
            
            // Additional fallback: check for header without emoji
            const hasHeaderNoEmoji = comment.body.includes('## ASR Benchmark Results');
            
            console.log(`Comment by ${comment.user.login}: isBot=${isBot}, hasIdentifier=${hasIdentifier}, hasHeader=${hasHeader}, hasHeaderNoEmoji=${hasHeaderNoEmoji}`);
            
            return isBot && (hasIdentifier || hasHeader || hasHeaderNoEmoji);
          });
          
          if (botComment) {
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: botComment.id,
              body: body
            });
            console.log('✅ Updated existing ASR benchmark comment');
          } else {
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: body
            });
            console.log('✅ Created new ASR benchmark comment');
          }
    
    # Check performance thresholds
    - name: Check Performance Thresholds
      run: |
        CLEAN_WER="${{ steps.benchmark_clean.outputs.CLEAN_WER_AVG }}"
        OTHER_WER="${{ steps.benchmark_other.outputs.OTHER_WER_AVG }}"
        MAX_FILES="${{ steps.test_size.outputs.MAX_FILES }}"
        
        # Adaptive thresholds based on test size
        if [[ "$MAX_FILES" -le 10 ]]; then
          CLEAN_THRESHOLD=25
          OTHER_THRESHOLD=35
        else
          CLEAN_THRESHOLD=10
          OTHER_THRESHOLD=20
        fi
        
        FAILED=false
        
        if (( $(echo "$CLEAN_WER > $CLEAN_THRESHOLD" | bc -l) )); then
          echo "❌ test-clean WER too high: $CLEAN_WER% (threshold: $CLEAN_THRESHOLD%)"
          FAILED=true
        fi
        
        if (( $(echo "$OTHER_WER > $OTHER_THRESHOLD" | bc -l) )); then
          echo "❌ test-other WER too high: $OTHER_WER% (threshold: $OTHER_THRESHOLD%)"
          FAILED=true
        fi
        
        if [ "$FAILED" = true ]; then
          exit 1
        else
          echo "✅ ASR benchmark passed all thresholds"
        fi
    
    # Upload results
    - name: Upload Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: asr-results
        path: |
          asr_results_clean.json
          asr_results_other.json
          benchmark_clean.log
          benchmark_other.log