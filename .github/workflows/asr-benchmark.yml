name: ASR Benchmark

on:
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      max_files:
        description: 'Number of files to test per dataset'
        required: false
        default: '100'

jobs:
  asr-benchmark:
    name: ASR Benchmark
    runs-on: macos-14  # M1 Mac for Neural Engine support
    permissions:
      contents: read
      pull-requests: write
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Setup Swift
      uses: swift-actions/setup-swift@v2
      with:
        swift-version: "6.0"
    
    # Cache Swift Package Manager
    - name: Cache SPM
      uses: actions/cache@v4
      with:
        path: .build
        key: ${{ runner.os }}-spm-${{ hashFiles('Package.resolved') }}
        restore-keys: |
          ${{ runner.os }}-spm-
    
    # Cache Parakeet Models (CoreML compiled models)
    - name: Cache Parakeet Models
      uses: actions/cache@v4
      with:
        path: ~/Library/Application Support/FluidAudio/Models/Parakeet
        key: ${{ runner.os }}-parakeet-mlmodelc-v1
        restore-keys: |
          ${{ runner.os }}-parakeet-mlmodelc-
    
    # Cache LibriSpeech Dataset
    - name: Cache LibriSpeech Dataset
      uses: actions/cache@v4
      with:
        path: ~/Documents/Datasets/librispeech
        key: ${{ runner.os }}-librispeech-test-clean-v1
    
    # Build in release mode
    - name: Build
      run: swift build -c release
    
    # Download Parakeet models if not cached
    - name: Download Parakeet Models
      run: |
        echo "üì• === PARAKEET MODEL DOWNLOAD STEP ==="
        echo "üì• Ensuring Parakeet models are available..."
        MODELS_DIR="$HOME/Library/Application Support/FluidAudio/Models/Parakeet"
        mkdir -p "$MODELS_DIR"
        
        echo "üìç Models directory: $MODELS_DIR"
        
        # Check if all required compiled model files exist
        if [ -d "$MODELS_DIR/Melspectogram.mlmodelc" ] && \
           [ -d "$MODELS_DIR/ParakeetEncoder.mlmodelc" ] && \
           [ -d "$MODELS_DIR/ParakeetDecoder.mlmodelc" ] && \
           [ -d "$MODELS_DIR/RNNTJoint.mlmodelc" ]; then
          echo "‚úÖ All Parakeet compiled models already cached"
          ls -la "$MODELS_DIR"
        else
          echo "üì• Downloading Parakeet models from Hugging Face..."
          
          # Download models from FluidInference/parakeet-tdt-0.6b-v2-coreml
          # Use git to clone the model repository
          echo "üì• Cloning model repository..."
          
          # Create a temporary directory for cloning
          TEMP_DIR=$(mktemp -d)
          cd "$TEMP_DIR"
          
          # Clone the repository with depth 1 to save time
          git clone --depth 1 https://huggingface.co/FluidInference/parakeet-tdt-0.6b-v2-coreml.git
          
          # Copy the mlpackage directories to the models directory and compile them
          cd parakeet-tdt-0.6b-v2-coreml
          for model in "Melspectogram" "ParakeetEncoder" "ParakeetDecoder" "RNNTJoint"; do
            if [ -d "${model}.mlpackage" ]; then
              echo "üì¶ Installing ${model}.mlpackage..."
              cp -r "${model}.mlpackage" "$MODELS_DIR/"
              echo "‚úÖ Installed ${model}.mlpackage"
            else
              echo "‚ùå ${model}.mlpackage not found in repository"
            fi
          done
          
          # Clean up
          cd /
          rm -rf "$TEMP_DIR"
          
          # Compile models to .mlmodelc format
          echo "üî® Compiling models to .mlmodelc format..."
          cd "$MODELS_DIR"
          
          # Use the fluidaudio CLI to download models if they exist
          echo "üîÑ Attempting to download pre-compiled models using fluidaudio CLI..."
          
          # Try to download models using the built CLI
          if swift run -c release fluidaudio download --dataset parakeet-models; then
            echo "‚úÖ Downloaded Parakeet models using CLI"
          else
            echo "‚ö†Ô∏è  CLI download failed, attempting manual compilation..."
            
            # Create a Swift script to compile models
            echo 'import CoreML' > compile_models.swift
            echo 'import Foundation' >> compile_models.swift
            echo '' >> compile_models.swift
            echo 'let modelsPath = FileManager.default.currentDirectoryPath' >> compile_models.swift
            echo 'let models = ["Melspectogram", "ParakeetEncoder", "ParakeetDecoder", "RNNTJoint"]' >> compile_models.swift
            echo '' >> compile_models.swift
            echo 'for modelName in models {' >> compile_models.swift
            echo '    let packagePath = URL(fileURLWithPath: "\(modelsPath)/\(modelName).mlpackage")' >> compile_models.swift
            echo '    let compiledPath = URL(fileURLWithPath: "\(modelsPath)/\(modelName).mlmodelc")' >> compile_models.swift
            echo '    ' >> compile_models.swift
            echo '    if FileManager.default.fileExists(atPath: compiledPath.path) {' >> compile_models.swift
            echo '        print("‚úÖ \(modelName).mlmodelc already exists")' >> compile_models.swift
            echo '        continue' >> compile_models.swift
            echo '    }' >> compile_models.swift
            echo '    ' >> compile_models.swift
            echo '    guard FileManager.default.fileExists(atPath: packagePath.path) else {' >> compile_models.swift
            echo '        print("‚ùå \(modelName).mlpackage not found")' >> compile_models.swift
            echo '        continue' >> compile_models.swift
            echo '    }' >> compile_models.swift
            echo '    ' >> compile_models.swift
            echo '    do {' >> compile_models.swift
            echo '        print("üì¶ Compiling \(modelName).mlpackage...")' >> compile_models.swift
            echo '        let compiledURL = try MLModel.compileModel(at: packagePath)' >> compile_models.swift
            echo '        try FileManager.default.moveItem(at: compiledURL, to: compiledPath)' >> compile_models.swift
            echo '        print("‚úÖ Successfully compiled \(modelName).mlmodelc")' >> compile_models.swift
            echo '    } catch {' >> compile_models.swift
            echo '        print("‚ùå Failed to compile \(modelName): \(error)")' >> compile_models.swift
            echo '    }' >> compile_models.swift
            echo '}' >> compile_models.swift
            
            swift compile_models.swift
            rm compile_models.swift
          fi
          
          # Remove the original .mlpackage files after compilation to save space
          for model in "Melspectogram" "ParakeetEncoder" "ParakeetDecoder" "RNNTJoint"; do
            if [ -d "$MODELS_DIR/${model}.mlpackage" ]; then
              echo "üßπ Removing ${model}.mlpackage after compilation"
              rm -rf "$MODELS_DIR/${model}.mlpackage"
            fi
          done
          
          # Verify all compiled models are present
          if [ -d "$MODELS_DIR/Melspectogram.mlmodelc" ] && \
             [ -d "$MODELS_DIR/ParakeetEncoder.mlmodelc" ] && \
             [ -d "$MODELS_DIR/ParakeetDecoder.mlmodelc" ] && \
             [ -d "$MODELS_DIR/RNNTJoint.mlmodelc" ]; then
            echo "‚úÖ All Parakeet models compiled successfully"
            ls -la "$MODELS_DIR"
          else
            echo "‚ùå Failed to compile all required models"
            echo "SKIP_BENCHMARK=true" >> $GITHUB_ENV
          fi
        fi
    
    # Determine test size based on changes
    - name: Determine Test Size
      id: test_size
      run: |
        # Default to 100 files for both datasets
        MAX_FILES="${{ github.event.inputs.max_files || '100' }}"
        
        # If only minor changes, do quick test
        if [[ "${{ github.event_name }}" == "pull_request" ]]; then
          # Fetch the base branch
          git fetch origin ${{ github.base_ref }}:refs/remotes/origin/${{ github.base_ref }} || true
          
          # Check what files changed
          CHANGED_FILES=$(git diff --name-only origin/${{ github.base_ref }}...HEAD 2>/dev/null || echo "")
          
          # If only docs/configs changed, do minimal test
          if echo "$CHANGED_FILES" | grep -qE "^(README|\.github/|docs/|.*\.md$)" && \
             ! echo "$CHANGED_FILES" | grep -qE "^Sources/"; then
            MAX_FILES=10
            echo "üìù Only documentation changed, running minimal test"
          # If ASR-related files changed, do full test
          elif echo "$CHANGED_FILES" | grep -qE "(ASR|Asr|asr|Parakeet|Transcript)"; then
            MAX_FILES=100
            echo "üîç ASR-related changes detected, running comprehensive test"
          else
            MAX_FILES=50
            echo "üìä Standard test size for general changes"
          fi
        fi
        
        echo "MAX_FILES=$MAX_FILES" >> $GITHUB_OUTPUT
        echo "üìä Will test with $MAX_FILES files per dataset"
    
    # Run ASR benchmarks in parallel
    - name: Run ASR Benchmarks - Parallel
      id: benchmark_parallel
      if: env.SKIP_BENCHMARK != 'true'
      run: |
        MAX_FILES="${{ steps.test_size.outputs.MAX_FILES }}"
        echo "Running ASR benchmarks in parallel with $MAX_FILES files each"
        
        # Start test-clean in background
        (
          swift run -c release fluidaudio asr-benchmark \
            --subset test-clean \
            --max-files "$MAX_FILES" \
            --output asr_results_clean.json 2>&1 | tee benchmark_clean.log
          
          # Extract WER metrics
          WER_AVG=$(grep "Average WER:" benchmark_clean.log | grep -o '[0-9.]*' | head -1 || echo "0")
          WER_MED=$(grep "Median WER:" benchmark_clean.log | grep -o '[0-9.]*' | head -1 || echo "0")
          
          # Save to temporary file for later collection
          echo "CLEAN_WER_AVG=$WER_AVG" > clean_metrics.txt
          echo "CLEAN_WER_MED=$WER_MED" >> clean_metrics.txt
        ) &
        CLEAN_PID=$!
        
        # Start test-other in background
        (
          swift run -c release fluidaudio asr-benchmark \
            --subset test-other \
            --max-files "$MAX_FILES" \
            --output asr_results_other.json 2>&1 | tee benchmark_other.log
          
          # Extract WER metrics
          WER_AVG=$(grep "Average WER:" benchmark_other.log | grep -o '[0-9.]*' | head -1 || echo "0")
          WER_MED=$(grep "Median WER:" benchmark_other.log | grep -o '[0-9.]*' | head -1 || echo "0")
          
          # Save to temporary file for later collection
          echo "OTHER_WER_AVG=$WER_AVG" > other_metrics.txt
          echo "OTHER_WER_MED=$WER_MED" >> other_metrics.txt
        ) &
        OTHER_PID=$!
        
        # Wait for both to complete
        echo "‚è≥ Waiting for benchmarks to complete..."
        wait $CLEAN_PID
        CLEAN_EXIT=$?
        wait $OTHER_PID
        OTHER_EXIT=$?
        
        # Check if both succeeded
        if [ $CLEAN_EXIT -ne 0 ] || [ $OTHER_EXIT -ne 0 ]; then
          echo "‚ùå One or more benchmarks failed"
          exit 1
        fi
        
        # Collect results and output to GitHub
        if [ -f clean_metrics.txt ]; then
          cat clean_metrics.txt >> $GITHUB_OUTPUT
        fi
        if [ -f other_metrics.txt ]; then
          cat other_metrics.txt >> $GITHUB_OUTPUT
        fi
        
        echo "‚úÖ Both benchmarks completed successfully"
    
    # Calculate combined metrics
    - name: Calculate Combined Metrics
      id: combined
      if: env.SKIP_BENCHMARK != 'true'
      run: |
        # Count files
        TOTAL_FILES=$(( ${{ steps.test_size.outputs.MAX_FILES }} * 2 ))
        
        echo "TOTAL_FILES=$TOTAL_FILES" >> $GITHUB_OUTPUT
    
    # Post comment with combined results
    - name: Comment PR with ASR Results
      if: github.event_name == 'pull_request' && always()
      continue-on-error: true
      uses: actions/github-script@v7
      with:
        script: |
          // Extract all metrics
          const cleanWerAvg = '${{ steps.benchmark_parallel.outputs.CLEAN_WER_AVG }}' || 'N/A';
          const cleanWerMed = '${{ steps.benchmark_parallel.outputs.CLEAN_WER_MED }}' || 'N/A';
          
          const otherWerAvg = '${{ steps.benchmark_parallel.outputs.OTHER_WER_AVG }}' || 'N/A';
          const otherWerMed = '${{ steps.benchmark_parallel.outputs.OTHER_WER_MED }}' || 'N/A';
          
          const totalFiles = '${{ steps.combined.outputs.TOTAL_FILES }}' || 'N/A';
          const filesPerDataset = '${{ steps.test_size.outputs.MAX_FILES }}';
          
          let body = `## üîä ASR Benchmark Results
          
          **Last Updated:** ${new Date().toISOString()}
          **Configuration:** test-clean & test-other | ${filesPerDataset} files per dataset | Total: ${totalFiles} files
          
          ### üìà Performance Metrics
          
          | Metric | test-clean | test-other | Delta | Status |
          |--------|------------|------------|-------|--------|
          | **WER Average** | ${cleanWerAvg}% | ${otherWerAvg}% | +${(parseFloat(otherWerAvg) - parseFloat(cleanWerAvg)).toFixed(1)}% | ${parseFloat(cleanWerAvg) < 10 && parseFloat(otherWerAvg) < 20 ? '‚úÖ' : '‚ö†Ô∏è'} |
          | **WER Median** | ${cleanWerMed}% | ${otherWerMed}% | +${(parseFloat(otherWerMed) - parseFloat(cleanWerMed)).toFixed(1)}% | ${parseFloat(cleanWerMed) < 5 && parseFloat(otherWerMed) < 10 ? '‚úÖ' : '‚ö†Ô∏è'} |
          
          ### üéØ Performance Targets
          
          | Target | Status | Result |
          |--------|--------|--------|
          | test-clean WER < 10% | ${parseFloat(cleanWerAvg) < 10 ? '‚úÖ PASS' : '‚ùå FAIL'} | ${cleanWerAvg}% |
          | test-other WER < 20% | ${parseFloat(otherWerAvg) < 20 ? '‚úÖ PASS' : '‚ùå FAIL'} | ${otherWerAvg}% |
          
          
          ---
          <sub>ü§ñ Automated ASR benchmark by FluidAudio CI</sub>
          
          <!-- fluidaudio-benchmark-asr -->`;
          
          // Find and update existing comment
          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
          });
          
          console.log(`Found ${comments.length} comments on PR`);
          
          const botComment = comments.find(comment => {
            // Check if it's a bot comment (GitHub Actions bot)
            const isBot = comment.user.type === 'Bot' || 
                         comment.user.login === 'github-actions[bot]' ||
                         comment.user.login.includes('[bot]');
            
            // Check if it contains our identifier
            const hasIdentifier = comment.body.includes('<!-- fluidaudio-benchmark-asr -->');
            
            // Also check for the header as a fallback (with emoji)
            const hasHeader = comment.body.includes('## üîä ASR Benchmark Results');
            
            // Additional fallback: check for header without emoji
            const hasHeaderNoEmoji = comment.body.includes('## ASR Benchmark Results');
            
            console.log(`Comment by ${comment.user.login}: isBot=${isBot}, hasIdentifier=${hasIdentifier}, hasHeader=${hasHeader}, hasHeaderNoEmoji=${hasHeaderNoEmoji}`);
            
            return isBot && (hasIdentifier || hasHeader || hasHeaderNoEmoji);
          });
          
          if (botComment) {
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: botComment.id,
              body: body
            });
            console.log('‚úÖ Updated existing ASR benchmark comment');
          } else {
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: body
            });
            console.log('‚úÖ Created new ASR benchmark comment');
          }
    
    # Check performance thresholds
    - name: Check Performance Thresholds
      run: |
        CLEAN_WER="${{ steps.benchmark_parallel.outputs.CLEAN_WER_AVG }}"
        OTHER_WER="${{ steps.benchmark_parallel.outputs.OTHER_WER_AVG }}"
        MAX_FILES="${{ steps.test_size.outputs.MAX_FILES }}"
        
        # Adaptive thresholds based on test size
        if [[ "$MAX_FILES" -le 10 ]]; then
          CLEAN_THRESHOLD=25
          OTHER_THRESHOLD=35
        else
          CLEAN_THRESHOLD=10
          OTHER_THRESHOLD=20
        fi
        
        FAILED=false
        
        if (( $(echo "$CLEAN_WER > $CLEAN_THRESHOLD" | bc -l) )); then
          echo "‚ùå test-clean WER too high: $CLEAN_WER% (threshold: $CLEAN_THRESHOLD%)"
          FAILED=true
        fi
        
        if (( $(echo "$OTHER_WER > $OTHER_THRESHOLD" | bc -l) )); then
          echo "‚ùå test-other WER too high: $OTHER_WER% (threshold: $OTHER_THRESHOLD%)"
          FAILED=true
        fi
        
        if [ "$FAILED" = true ]; then
          exit 1
        else
          echo "‚úÖ ASR benchmark passed all thresholds"
        fi
    
    # Upload results
    - name: Upload Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: asr-results
        path: |
          asr_results_clean.json
          asr_results_other.json
          benchmark_clean.log
          benchmark_other.log