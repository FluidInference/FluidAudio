name: VAD Benchmark

on:
  pull_request:
    branches: [ main ]
  schedule:
    # Run daily at 6 AM UTC
    - cron: '0 6 * * *'
  workflow_dispatch:
    inputs:
      num_files:
        description: 'Number of test files (default: 40)'
        required: false
        default: '40'
        type: string

permissions:
  contents: read
  pull-requests: write
  issues: write


jobs:
  vad-benchmark:
    runs-on: macos-latest
    timeout-minutes: 45

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch full git history
        lfs: false      # Don't use LFS since our files are small

    - name: Verify checkout
      run: |
        echo "üîç Immediate post-checkout verification:"
        echo "Branch: $(git branch --show-current)"
        echo "Commit: $(git rev-parse HEAD)"
        echo "Commit message: $(git log -1 --pretty=format:'%s')"
        echo ""
        echo "Directory contents:"
        ls -la
        echo ""
        echo "‚ÑπÔ∏è VAD models will be downloaded automatically from Hugging Face"

    - name: Setup Swift 6.1
      uses: swift-actions/setup-swift@v2
      with:
        swift-version: "6.1"

    - name: Cache Swift packages
      uses: actions/cache@v4
      with:
        path: |
          .build
          ~/Library/Caches/org.swift.swiftpm
        key: ${{ runner.os }}-swift-6.1-${{ hashFiles('Package.swift') }}
        restore-keys: |
          ${{ runner.os }}-swift-6.1-

    - name: Cache VAD models
      uses: actions/cache@v4
      with:
        path: |
          ~/Library/Application Support/FluidAudio/vad
        key: ${{ runner.os }}-vad-models-v4
        restore-keys: |
          ${{ runner.os }}-vad-models-

    - name: Cache test audio data
      uses: actions/cache@v4
      with:
        path: /tmp/vad_test_data
        key: ${{ runner.os }}-vad-test-data-${{ github.event.inputs.num_files || '40' }}
        restore-keys: |
          ${{ runner.os }}-vad-test-data-

    - name: Check models directory
      run: |
        echo "üìÅ Checking for VAD models..."
        echo "üìÇ Current directory contents:"
        ls -la
        echo ""
        echo "üîç Git status and recent commits:"
        git log --oneline -5
        echo ""
        echo "‚ÑπÔ∏è VAD models will be downloaded automatically from Hugging Face"
        echo "   Model Repository: alexwengg/coreml_silero_vad"
        echo "   Test Data Repository: alexwengg/musan_mini2 (smaller files, faster CI)"
        echo "   Models: silero_stft.mlmodel, silero_encoder.mlmodel, silero_rnn_decoder.mlmodel, correct_classifier_conv1d.mlmodel"
        echo "   üéØ Enhanced Features: SNR filtering, spectral analysis, noise detection"

    - name: Install system dependencies
      run: |
        # Install any system dependencies if needed
        echo "üì¶ Installing system dependencies..."
        # brew install ffmpeg || true  # Uncomment if needed for audio processing

    - name: Build Swift package
      run: |
        echo "üî® Building Swift package..."
        echo "Swift version: $(swift --version)"
        echo "Xcode version: $(xcodebuild -version)"

        # Clean any existing build artifacts
        swift package clean

        # Build with explicit configuration
        swift build --configuration release

    - name: Prepare test environment
      run: |
        echo "üì• Preparing test environment..."

        # Create test data directory
        mkdir -p /tmp/vad_test_data

        # Test files will be loaded by VAD benchmark in priority order:
        # 1. Local datasets (if VADDataset/ directory exists)
        # 2. Remote downloads from public URLs
        echo "‚úÖ Test environment prepared"
        echo "    Test data sources (in priority order):"
        echo "    1. Local datasets (VADDataset/ directory)"
        echo "    2. Remote downloads from public URLs:"
        echo "       - www2.cs.uic.edu (speech and music samples)"

    - name: Run VAD Benchmark Tests
      run: |
        echo "üöÄ Running VAD Benchmark Tests..."

        # Set test parameters
        NUM_FILES="${{ github.event.inputs.num_files || '40' }}"
        echo "üìä Running benchmark with $NUM_FILES test files"

        echo "‚ÑπÔ∏è VAD models and test data will be downloaded automatically from Hugging Face"
        echo "   Using musan_mini2 dataset for faster CI performance"
        echo "   This is the expected behavior - no pre-existing files required"

        # Run VAD benchmark using the vad-benchmark command
        echo ""
        echo "üî¨ Running CoreML VAD benchmark..."
        echo "=================================="

        set +e  # Don't exit on error
        # Using optimal threshold 0.3 with enhanced SNR filtering and spectral features
        # Enhanced VAD achieves F1-Score 76.9% (up from original 64.4%) with aggressive noise filtering
        swift run fluidaudio vad-benchmark \
          --num-files "$NUM_FILES" \
          --threshold 0.3

        BENCHMARK_EXIT_CODE=$?
        set -e  # Re-enable exit on error

        if [ $BENCHMARK_EXIT_CODE -eq 0 ]; then
          echo "‚úÖ VAD Benchmark completed successfully"
          echo "BENCHMARK_STATUS=SUCCESS" >> $GITHUB_ENV
        else
          echo "‚ùå VAD Benchmark failed with exit code $BENCHMARK_EXIT_CODE"
          echo "BENCHMARK_STATUS=FAILED" >> $GITHUB_ENV
          # Don't exit here - let the validation step handle it
        fi

    - name: Collect test results
      if: always()
      run: |
        echo "üìä Collecting test results..."

        # Look for benchmark results file
        if [ -f "vad_benchmark_results.json" ]; then
          echo "‚úÖ Found VAD benchmark results:"
          cat vad_benchmark_results.json | jq . || cat vad_benchmark_results.json
        else
          echo "‚ö†Ô∏è No benchmark results file found"
          echo "Expected file: vad_benchmark_results.json"
        fi

        # Look for test logs
        find . -name "*.log" -o -name "*test*" -type f 2>/dev/null | head -10 | while read logfile; do
          echo "üìÑ Found log: $logfile"
        done

    - name: Generate test report
      if: always()
      run: |
        echo "üìã Generating test report..."

        # Create a simple test report
        cat > test_report.md << 'EOF'
        # VAD Benchmark Test Report

        **Run Date:** $(date)
        **Branch:** ${{ github.ref_name }}
        **Commit:** ${{ github.sha }}
        **Test Files:** ${{ github.event.inputs.num_files || '40' }}

        ## Results

        EOF

        if [ -f "vad_benchmark_results.json" ]; then
          echo "### Benchmark Metrics" >> test_report.md
          echo "" >> test_report.md
          echo '```json' >> test_report.md
          cat vad_benchmark_results.json >> test_report.md
          echo '```' >> test_report.md
        else
          echo "### ‚ö†Ô∏è Results Missing" >> test_report.md
          echo "No benchmark results file was generated." >> test_report.md
        fi

        echo "" >> test_report.md
        echo "## Environment" >> test_report.md
        echo "- **OS:** macOS (GitHub Actions)" >> test_report.md
        echo "- **Xcode:** 16.1" >> test_report.md
        echo "- **Swift:** $(swift --version | head -1)" >> test_report.md

        echo "‚úÖ Test report generated: test_report.md"

    - name: Upload test artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: vad-benchmark-results-${{ github.sha }}
        path: |
          vad_benchmark_results.json
          test_report.md
          .build/debug/codecov/*.json
        retention-days: 30

    - name: Comment PR with results
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');

          let reportContent = '## üéØ VAD Benchmark Results\n\n';

          try {
            if (fs.existsSync('vad_benchmark_results.json')) {
              const results = JSON.parse(fs.readFileSync('vad_benchmark_results.json', 'utf8'));

              reportContent += `### üìä CoreML VAD Performance\n\n`;
              reportContent += `| Metric | Value |\n`;
              reportContent += `|--------|---------|\n`;
              reportContent += `| **Accuracy** | ${results.accuracy?.toFixed(1) || 'N/A'}% |\n`;
              reportContent += `| **Precision** | ${results.precision?.toFixed(1) || 'N/A'}% |\n`;
              reportContent += `| **Recall** | ${results.recall?.toFixed(1) || 'N/A'}% |\n`;
              reportContent += `| **F1-Score** | ${results.f1_score?.toFixed(1) || 'N/A'}% |\n`;
              reportContent += `| **Processing Time** | ${results.processing_time_seconds?.toFixed(1) || 'N/A'}s |\n`;
              reportContent += `| **Files Processed** | ${results.total_files || 'N/A'} |\n\n`;

              // Performance assessment
              const f1Score = results.f1_score || 0;
              if (f1Score >= 70) {
                reportContent += `### ‚úÖ Status: EXCELLENT\n`;
                reportContent += `F1-Score of ${f1Score.toFixed(1)}% meets quality standards.\n\n`;
              } else if (f1Score >= 60) {
                reportContent += `### ‚ö†Ô∏è Status: ACCEPTABLE\n`;
                reportContent += `F1-Score of ${f1Score.toFixed(1)}% is acceptable but could be improved.\n\n`;
              } else {
                reportContent += `### ‚ùå Status: NEEDS IMPROVEMENT\n`;
                reportContent += `F1-Score of ${f1Score.toFixed(1)}% is below target (60%+).\n\n`;
              }

            } else {
              reportContent += `### ‚ùå Test Failed\n\n`;
              reportContent += `No benchmark results were generated. Check the logs for details.\n\n`;
            }
          } catch (error) {
            reportContent += `### ‚ö†Ô∏è Error Processing Results\n\n`;
            reportContent += `Could not parse benchmark results: ${error.message}\n\n`;
          }

          reportContent += `### üîó Links\n`;
          reportContent += `- [Workflow Run](${context.payload.repository.html_url}/actions/runs/${{ github.run_id }})\n`;
          reportContent += `- [Test Artifacts](${context.payload.repository.html_url}/actions/runs/${{ github.run_id }}#artifacts)\n`;

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: reportContent
          });

    - name: Validate performance thresholds
      if: always()
      run: |
        echo "üéØ Validating performance thresholds..."

        if [ -f "vad_benchmark_results.json" ]; then
          echo "üìä Validating CoreML VAD benchmark results..."

          # Check if benchmark was skipped
          STATUS=$(grep '"status"' vad_benchmark_results.json | sed 's/.*"status"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/' | head -1 || echo "unknown")

          if [ "$STATUS" = "skipped" ]; then
            echo "‚è≠Ô∏è Benchmark was skipped - no performance validation needed"
            REASON=$(grep '"reason"' vad_benchmark_results.json | sed 's/.*"reason"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/' | head -1 || echo "Unknown reason")
            echo "   Reason: $REASON"
            echo "BENCHMARK_STATUS=SKIPPED" >> $GITHUB_ENV
            # Don't exit with error for skipped benchmarks
          else
            # Extract metrics using simple grep/sed (avoiding jq dependency)
            ACCURACY=$(grep '"accuracy"' vad_benchmark_results.json | sed 's/.*: *\([0-9.]*\).*/\1/' | head -1 || echo "0")
            F1_SCORE=$(grep '"f1_score"' vad_benchmark_results.json | sed 's/.*: *\([0-9.]*\).*/\1/' | head -1 || echo "0")

            echo "üìä Performance Summary:"
            echo "   CoreML VAD - F1-Score: ${F1_SCORE}%, Accuracy: ${ACCURACY}%"

            # Set performance thresholds (F1-score is primary metric for VAD)
            # Updated thresholds based on enhanced VAD performance improvements
            MIN_F1_SCORE=70.0

            # Validate thresholds (using bc for floating point comparison)
            if command -v bc >/dev/null 2>&1; then
              F1_MEETS_THRESHOLD=$(echo "$F1_SCORE >= $MIN_F1_SCORE" | bc -l)

              if (( $F1_MEETS_THRESHOLD )); then
                echo "‚úÖ Performance thresholds met!"
                echo "   F1-Score: $F1_SCORE% (‚â• $MIN_F1_SCORE%) ‚úì"
                echo "BENCHMARK_STATUS=PASSED" >> $GITHUB_ENV
              else
                echo "‚ùå Performance below thresholds:"
                echo "   F1-Score: $F1_SCORE% (min: $MIN_F1_SCORE%)"
                echo "BENCHMARK_STATUS=FAILED" >> $GITHUB_ENV
                exit 1
              fi
            else
              echo "‚ö†Ô∏è Cannot validate thresholds (bc not available)"
              echo "BENCHMARK_STATUS=UNKNOWN" >> $GITHUB_ENV
            fi
          fi
        else
          echo "‚ùå No results file to validate"
          echo "Expected file: vad_benchmark_results.json"

          # Check if this was due to a benchmark failure vs missing results
          if [ "${BENCHMARK_STATUS:-}" = "FAILED" ]; then
            echo "   Benchmark failed during execution"
            echo "   Possible causes: network issues, missing datasets, model compilation problems"
            echo "   This is not necessarily a performance regression"
            echo "BENCHMARK_STATUS=EXECUTION_FAILED" >> $GITHUB_ENV
            # Don't exit with error for infrastructure failures
          else
            echo "   No benchmark was attempted"
            echo "BENCHMARK_STATUS=NO_RESULTS" >> $GITHUB_ENV
            exit 1
          fi
        fi
