name: Qwen3-ASR Benchmark

on:
  pull_request:
    branches: [main]
  workflow_dispatch:

jobs:
  qwen3-asr-benchmark:
    name: Qwen3-ASR Benchmark
    runs-on: macos-15
    permissions:
      contents: read
      pull-requests: write

    steps:
      - uses: actions/checkout@v5

      - uses: swift-actions/setup-swift@v2
        with:
          swift-version: "6.1"

      - name: Cache Dependencies
        uses: actions/cache@v4
        with:
          path: |
            .build
            ~/Library/Application Support/FluidAudio/Models/qwen3-asr-0.6b-coreml
            ~/Library/Application Support/FluidAudio/Datasets/LibriSpeech
            ~/Library/Caches/Homebrew
            /usr/local/Cellar/ffmpeg
            /opt/homebrew/Cellar/ffmpeg
          key: ${{ runner.os }}-qwen3-asr-${{ hashFiles('Package.resolved', 'Sources/FluidAudio/Frameworks/**', 'Sources/FluidAudio/ModelRegistry.swift', 'Sources/FluidAudio/ModelNames.swift') }}

      - name: Install ffmpeg
        run: |
          brew install ffmpeg || echo "ffmpeg may already be installed"
          ffmpeg -version || echo "ffmpeg not available"

      - name: Build
        run: swift build -c release

      - name: Run Benchmarks
        id: benchmark
        run: |
          MAX_FILES="25"
          BENCHMARK_START=$(date +%s)

          set -o pipefail

          run_benchmark() {
            local VARIANT=$1
            local MAX=$2
            local OUTPUT=$3

            echo "========================================="
            echo "Running Qwen3-ASR benchmark: $VARIANT (max $MAX files)"
            echo "Output: $OUTPUT"
            echo "========================================="

            if swift run fluidaudiocli qwen3-benchmark \
              --variant "$VARIANT" --max-files "$MAX" \
              --output "$OUTPUT" > benchmark_log.txt 2>&1; then
              echo "Benchmark $VARIANT completed successfully"
              return 0
            else
              echo "Benchmark $VARIANT FAILED with exit code $?"
              echo "Full output:"
              cat benchmark_log.txt
              return 1
            fi
          }

          # Run both variants
          run_benchmark "f32" "$MAX_FILES" "qwen3_results_f32.json" || F32_FAILED=1
          run_benchmark "int8" "$MAX_FILES" "qwen3_results_int8.json" || INT8_FAILED=1

          # Extract f32 metrics
          if [ -f qwen3_results_f32.json ]; then
            F32_WER_AVG=$(jq -r '.summary.averageWER * 100' qwen3_results_f32.json 2>/dev/null)
            F32_WER_MED=$(jq -r '.summary.medianWER * 100' qwen3_results_f32.json 2>/dev/null)
            F32_CER_AVG=$(jq -r '.summary.averageCER * 100' qwen3_results_f32.json 2>/dev/null)
            F32_RTFx=$(jq -r '.summary.medianRTFx' qwen3_results_f32.json 2>/dev/null)
            F32_OVERALL_RTFx=$(jq -r '.summary.overallRTFx' qwen3_results_f32.json 2>/dev/null)

            [ "$F32_WER_AVG" != "null" ] && [ -n "$F32_WER_AVG" ] && F32_WER_AVG=$(printf "%.2f" "$F32_WER_AVG") || F32_WER_AVG="N/A"
            [ "$F32_WER_MED" != "null" ] && [ -n "$F32_WER_MED" ] && F32_WER_MED=$(printf "%.2f" "$F32_WER_MED") || F32_WER_MED="N/A"
            [ "$F32_CER_AVG" != "null" ] && [ -n "$F32_CER_AVG" ] && F32_CER_AVG=$(printf "%.2f" "$F32_CER_AVG") || F32_CER_AVG="N/A"
            [ "$F32_RTFx" != "null" ] && [ -n "$F32_RTFx" ] && F32_RTFx=$(printf "%.2f" "$F32_RTFx") || F32_RTFx="N/A"
            [ "$F32_OVERALL_RTFx" != "null" ] && [ -n "$F32_OVERALL_RTFx" ] && F32_OVERALL_RTFx=$(printf "%.2f" "$F32_OVERALL_RTFx") || F32_OVERALL_RTFx="N/A"
          fi

          # Extract int8 metrics
          if [ -f qwen3_results_int8.json ]; then
            INT8_WER_AVG=$(jq -r '.summary.averageWER * 100' qwen3_results_int8.json 2>/dev/null)
            INT8_WER_MED=$(jq -r '.summary.medianWER * 100' qwen3_results_int8.json 2>/dev/null)
            INT8_CER_AVG=$(jq -r '.summary.averageCER * 100' qwen3_results_int8.json 2>/dev/null)
            INT8_RTFx=$(jq -r '.summary.medianRTFx' qwen3_results_int8.json 2>/dev/null)
            INT8_OVERALL_RTFx=$(jq -r '.summary.overallRTFx' qwen3_results_int8.json 2>/dev/null)

            [ "$INT8_WER_AVG" != "null" ] && [ -n "$INT8_WER_AVG" ] && INT8_WER_AVG=$(printf "%.2f" "$INT8_WER_AVG") || INT8_WER_AVG="N/A"
            [ "$INT8_WER_MED" != "null" ] && [ -n "$INT8_WER_MED" ] && INT8_WER_MED=$(printf "%.2f" "$INT8_WER_MED") || INT8_WER_MED="N/A"
            [ "$INT8_CER_AVG" != "null" ] && [ -n "$INT8_CER_AVG" ] && INT8_CER_AVG=$(printf "%.2f" "$INT8_CER_AVG") || INT8_CER_AVG="N/A"
            [ "$INT8_RTFx" != "null" ] && [ -n "$INT8_RTFx" ] && INT8_RTFx=$(printf "%.2f" "$INT8_RTFx") || INT8_RTFx="N/A"
            [ "$INT8_OVERALL_RTFx" != "null" ] && [ -n "$INT8_OVERALL_RTFx" ] && INT8_OVERALL_RTFx=$(printf "%.2f" "$INT8_OVERALL_RTFx") || INT8_OVERALL_RTFx="N/A"
          fi

          # Output metrics
          echo "F32_WER_AVG=${F32_WER_AVG:-N/A}" >> $GITHUB_OUTPUT
          echo "F32_WER_MED=${F32_WER_MED:-N/A}" >> $GITHUB_OUTPUT
          echo "F32_CER_AVG=${F32_CER_AVG:-N/A}" >> $GITHUB_OUTPUT
          echo "F32_RTFx=${F32_RTFx:-N/A}" >> $GITHUB_OUTPUT
          echo "F32_OVERALL_RTFx=${F32_OVERALL_RTFx:-N/A}" >> $GITHUB_OUTPUT
          echo "INT8_WER_AVG=${INT8_WER_AVG:-N/A}" >> $GITHUB_OUTPUT
          echo "INT8_WER_MED=${INT8_WER_MED:-N/A}" >> $GITHUB_OUTPUT
          echo "INT8_CER_AVG=${INT8_CER_AVG:-N/A}" >> $GITHUB_OUTPUT
          echo "INT8_RTFx=${INT8_RTFx:-N/A}" >> $GITHUB_OUTPUT
          echo "INT8_OVERALL_RTFx=${INT8_OVERALL_RTFx:-N/A}" >> $GITHUB_OUTPUT

          EXECUTION_TIME=$(( ($(date +%s) - BENCHMARK_START) / 60 ))m$(( ($(date +%s) - BENCHMARK_START) % 60 ))s
          echo "EXECUTION_TIME=$EXECUTION_TIME" >> $GITHUB_OUTPUT
          echo "FILES_COUNT=$MAX_FILES" >> $GITHUB_OUTPUT

          if [ ! -z "$F32_FAILED" ] || [ ! -z "$INT8_FAILED" ]; then
            echo "BENCHMARK_STATUS=PARTIAL_FAILURE" >> $GITHUB_OUTPUT
            echo "Some benchmarks failed:"
            [ ! -z "$F32_FAILED" ] && echo "  - f32 benchmark failed"
            [ ! -z "$INT8_FAILED" ] && echo "  - int8 benchmark failed"
          else
            echo "BENCHMARK_STATUS=SUCCESS" >> $GITHUB_OUTPUT
            echo "All benchmarks completed successfully"
          fi

      - name: Comment PR
        if: github.event_name == 'pull_request'
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          script: |
            const benchmarkStatus = '${{ steps.benchmark.outputs.BENCHMARK_STATUS }}';
            const statusEmoji = benchmarkStatus === 'SUCCESS' ? '✅' : '⚠️';
            const statusText = benchmarkStatus === 'SUCCESS' ? 'All benchmarks passed' : 'Some benchmarks failed (see logs)';

            const body = `## Qwen3-ASR Benchmark Results ${statusEmoji}

            **Status:** ${statusText}

            ### LibriSpeech test-clean

            | Variant | WER Avg | WER Med | CER Avg | Median RTFx | Overall RTFx | Status |
            |---------|---------|---------|---------|-------------|--------------|--------|
            | f32 (FP16) | ${{ steps.benchmark.outputs.F32_WER_AVG }}% | ${{ steps.benchmark.outputs.F32_WER_MED }}% | ${{ steps.benchmark.outputs.F32_CER_AVG }}% | ${{ steps.benchmark.outputs.F32_RTFx }}x | ${{ steps.benchmark.outputs.F32_OVERALL_RTFx }}x | ${parseFloat('${{ steps.benchmark.outputs.F32_WER_AVG }}') < 10 ? '✅' : '${{ steps.benchmark.outputs.F32_WER_AVG }}' === 'N/A' ? '❌' : '⚠️'} |
            | int8 | ${{ steps.benchmark.outputs.INT8_WER_AVG }}% | ${{ steps.benchmark.outputs.INT8_WER_MED }}% | ${{ steps.benchmark.outputs.INT8_CER_AVG }}% | ${{ steps.benchmark.outputs.INT8_RTFx }}x | ${{ steps.benchmark.outputs.INT8_OVERALL_RTFx }}x | ${parseFloat('${{ steps.benchmark.outputs.INT8_WER_AVG }}') < 10 ? '✅' : '${{ steps.benchmark.outputs.INT8_WER_AVG }}' === 'N/A' ? '❌' : '⚠️'} |

            ### Model Sizes

            | Variant | Decoder | Total (approx) |
            |---------|---------|----------------|
            | f32 (FP16) | 1.1 GB | ~1.75 GB |
            | int8 | 571 MB | ~900 MB |

            <sub>${{ steps.benchmark.outputs.FILES_COUNT }} files per variant • Test runtime: ${{ steps.benchmark.outputs.EXECUTION_TIME }} • ${new Date().toLocaleString('en-US', { timeZone: 'America/New_York', year: 'numeric', month: '2-digit', day: '2-digit', hour: '2-digit', minute: '2-digit', hour12: true })} EST</sub>

            <sub>**RTFx** = Real-Time Factor (higher is better) • Audio duration / Processing time</sub>

            <!-- fluidaudio-benchmark-qwen3-asr -->`;

            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const existing = comments.find(c =>
              c.body.includes('<!-- fluidaudio-benchmark-qwen3-asr -->')
            );

            if (existing) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existing.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: qwen3-asr-results
          path: qwen3_results_*.json
