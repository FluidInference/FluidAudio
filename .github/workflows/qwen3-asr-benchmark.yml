name: Qwen3-ASR Benchmark

on:
  pull_request:
    branches: [main]
  workflow_dispatch:

jobs:
  qwen3-asr-benchmark:
    name: Qwen3-ASR Benchmark (int8)
    runs-on: macos-15
    permissions:
      contents: read
      pull-requests: write

    timeout-minutes: 45

    steps:
      - uses: actions/checkout@v5

      - uses: swift-actions/setup-swift@v2
        with:
          swift-version: "6.1"

      - name: Cache Dependencies
        uses: actions/cache@v4
        with:
          path: |
            .build
            ~/Library/Application Support/FluidAudio/Models/qwen3-asr-0.6b-coreml
            ~/Library/Application Support/FluidAudio/Datasets/LibriSpeech
            ~/Library/Caches/Homebrew
            /usr/local/Cellar/ffmpeg
            /opt/homebrew/Cellar/ffmpeg
          key: ${{ runner.os }}-qwen3-asr-${{ hashFiles('Package.resolved', 'Sources/FluidAudio/Frameworks/**', 'Sources/FluidAudio/ModelRegistry.swift', 'Sources/FluidAudio/ModelNames.swift') }}

      - name: Install ffmpeg
        run: |
          brew install ffmpeg || echo "ffmpeg may already be installed"
          ffmpeg -version || echo "ffmpeg not available"

      - name: Build
        run: swift build -c release

      - name: Run Benchmark
        id: benchmark
        run: |
          MAX_FILES="5"
          BENCHMARK_START=$(date +%s)

          set -o pipefail

          echo "========================================="
          echo "Running Qwen3-ASR int8 benchmark (max $MAX_FILES files)"
          echo "========================================="

          if swift run fluidaudiocli qwen3-benchmark \
            --variant int8 --max-files "$MAX_FILES" \
            --output qwen3_results_int8.json > benchmark_log.txt 2>&1; then
            echo "Benchmark completed successfully"
            echo "BENCHMARK_STATUS=SUCCESS" >> $GITHUB_OUTPUT
          else
            echo "Benchmark FAILED with exit code $?"
            cat benchmark_log.txt
            echo "BENCHMARK_STATUS=FAILED" >> $GITHUB_OUTPUT
          fi

          if [ -f qwen3_results_int8.json ]; then
            WER_AVG=$(jq -r '.summary.averageWER * 100' qwen3_results_int8.json 2>/dev/null)
            WER_MED=$(jq -r '.summary.medianWER * 100' qwen3_results_int8.json 2>/dev/null)
            CER_AVG=$(jq -r '.summary.averageCER * 100' qwen3_results_int8.json 2>/dev/null)
            RTFx=$(jq -r '.summary.medianRTFx' qwen3_results_int8.json 2>/dev/null)
            OVERALL_RTFx=$(jq -r '.summary.overallRTFx' qwen3_results_int8.json 2>/dev/null)

            [ "$WER_AVG" != "null" ] && [ -n "$WER_AVG" ] && WER_AVG=$(printf "%.2f" "$WER_AVG") || WER_AVG="N/A"
            [ "$WER_MED" != "null" ] && [ -n "$WER_MED" ] && WER_MED=$(printf "%.2f" "$WER_MED") || WER_MED="N/A"
            [ "$CER_AVG" != "null" ] && [ -n "$CER_AVG" ] && CER_AVG=$(printf "%.2f" "$CER_AVG") || CER_AVG="N/A"
            [ "$RTFx" != "null" ] && [ -n "$RTFx" ] && RTFx=$(printf "%.2f" "$RTFx") || RTFx="N/A"
            [ "$OVERALL_RTFx" != "null" ] && [ -n "$OVERALL_RTFx" ] && OVERALL_RTFx=$(printf "%.2f" "$OVERALL_RTFx") || OVERALL_RTFx="N/A"
          fi

          echo "WER_AVG=${WER_AVG:-N/A}" >> $GITHUB_OUTPUT
          echo "WER_MED=${WER_MED:-N/A}" >> $GITHUB_OUTPUT
          echo "CER_AVG=${CER_AVG:-N/A}" >> $GITHUB_OUTPUT
          echo "RTFx=${RTFx:-N/A}" >> $GITHUB_OUTPUT
          echo "OVERALL_RTFx=${OVERALL_RTFx:-N/A}" >> $GITHUB_OUTPUT

          EXECUTION_TIME=$(( ($(date +%s) - BENCHMARK_START) / 60 ))m$(( ($(date +%s) - BENCHMARK_START) % 60 ))s
          echo "EXECUTION_TIME=$EXECUTION_TIME" >> $GITHUB_OUTPUT
          echo "FILES_COUNT=$MAX_FILES" >> $GITHUB_OUTPUT

      - name: Comment PR
        if: github.event_name == 'pull_request'
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          script: |
            const status = '${{ steps.benchmark.outputs.BENCHMARK_STATUS }}';
            const emoji = status === 'SUCCESS' ? '✅' : '❌';

            const body = `## Qwen3-ASR int8 Benchmark ${emoji}

            | Metric | Value |
            |--------|-------|
            | WER Avg | ${{ steps.benchmark.outputs.WER_AVG }}% |
            | WER Med | ${{ steps.benchmark.outputs.WER_MED }}% |
            | CER Avg | ${{ steps.benchmark.outputs.CER_AVG }}% |
            | Median RTFx | ${{ steps.benchmark.outputs.RTFx }}x |
            | Overall RTFx | ${{ steps.benchmark.outputs.OVERALL_RTFx }}x |
            | Decoder size | 571 MB (vs 1.1 GB f32) |

            <sub>${{ steps.benchmark.outputs.FILES_COUNT }} files • LibriSpeech test-clean • Runtime: ${{ steps.benchmark.outputs.EXECUTION_TIME }}</sub>

            <!-- fluidaudio-benchmark-qwen3-asr -->`;

            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const existing = comments.find(c =>
              c.body.includes('<!-- fluidaudio-benchmark-qwen3-asr -->')
            );

            if (existing) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existing.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: qwen3-asr-results
          path: qwen3_results_*.json
